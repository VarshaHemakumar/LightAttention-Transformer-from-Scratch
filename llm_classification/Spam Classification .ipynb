{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Spam Classification using Encoder LLMs with Linear Probing [5 points]\nIn this part, we will use encoder Large Language Models (LLMs) for spam classification. We will leverage the rich features of pre-trained LLMs without fine-tuning them. Instead, we will freeze the LLM weights and train a lightweight classifier head (MLP) on top for spam classification.\n\n**Dataset:** Enron Spam Dataset\n\n**Expected Performance (Best Model):** {Accuracy: >85%, F1: >85%, Precision: >85%, Recall: >82%}","metadata":{}},{"cell_type":"markdown","source":"1. Load the Enron Spam dataset. Use the train/val/test splits and tokenize the text using your pre-trained LLMâ€™s tokenizer. Use your best judgement for the relevant input fields.","metadata":{}},{"cell_type":"code","source":"### ADD YOUR CODE HERE ###\n# Load Enron Spam dataset (consider using Hugging Face Datasets or manual loading if necessary)\n# Implement train/val/test splits\n# Tokenize text data using the chosen LLM's tokenizer\nfrom datasets import load_dataset\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer\nfrom datasets import Dataset\n\n# Loading the Enron Spam dataset from Hugging Face\ndataset = load_dataset(\"SetFit/enron_spam\")\n\n# Splitting the dataset\ntrain_df = dataset['train'].to_pandas()\ntest_df = dataset['test'].to_pandas()\n\n\ntrain_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n\n\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T23:29:05.494209Z","iopub.execute_input":"2025-04-09T23:29:05.494400Z","iopub.status.idle":"2025-04-09T23:29:23.976682Z","shell.execute_reply.started":"2025-04-09T23:29:05.494381Z","shell.execute_reply":"2025-04-09T23:29:23.975708Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/176 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"890408e5753d4bceb6377492aecd2f5a"}},"metadata":{}},{"name":"stderr","text":"Repo card metadata block was not found. Setting CardData to empty.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"train.jsonl:   0%|          | 0.00/101M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87d5fcc3e6ac4a0aacd601798df0ddc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.jsonl:   0%|          | 0.00/6.27M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28e233df1c4145b28c915720900690c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/31716 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf6a7386b53d4c449ff4f66cc2eaa82d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c84c1dfd2e4a46038caaf5a95c3833c6"}},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Tokenizing\ntokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n\n\ndef tokenize_function(examples):\n   \n    return tokenizer(examples['text'], padding='max_length', truncation=True)\n\n\ntokenized_train = tokenized_train.map(tokenize_function, batched=True)\ntokenized_val = tokenized_val.map(tokenize_function, batched=True)\ntokenized_test = tokenized_test.map(tokenize_function, batched=True)\n\n\ntokenized_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\ntokenized_val.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\ntokenized_test.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n\n\nprint(tokenized_train[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:20:05.392115Z","iopub.execute_input":"2025-04-09T21:20:05.392446Z","iopub.status.idle":"2025-04-09T21:20:26.302857Z","shell.execute_reply.started":"2025-04-09T21:20:05.392416Z","shell.execute_reply":"2025-04-09T21:20:26.302161Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/28544 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"883dec1bd3584ea0ba1f9912b99f3244"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3172 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f4f650079654e15aafa91da0956d72c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b014a93f3ce4f108f14abea9e9aed93"}},"metadata":{}},{"name":"stdout","text":"{'label': tensor(0), 'input_ids': tensor([  101,  3531,  3602,  2008,  2026, 10373,  4769,  2038,  2042,  2904,\n         2000,  1024,  1046,  4819,  3051,  1030,  3891,  5880,  2015,  1012,\n         4012,  3531, 10651,  2115,  4769,  2808,  1012,  4283,  1012,  8963,\n         1012,  3622,  1024,  1009,  4008,  1006,  1014,  1007,  2322,  6356,\n         2620,  2549,  5818, 27531,  7479,  1012,  3891,  5880,  2015,  1012,\n         4012,  1011,  2012, 19646,  1012,  1044, 21246,   102,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0])}\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"2. Model Setup â€“ Probing:","metadata":{}},{"cell_type":"markdown","source":"   a. Load a pre-trained LLM (e.g., DistilBERT, BART-encoder) for sequence classification. Choose a lightweight encoder model that is amenable to your GPU size. Consider using DistilBERT, TinyBERT, MobileBERT, AlBERT, or others. **Specify the chosen LLM below.**","metadata":{}},{"cell_type":"markdown","source":"   **Chosen Encoder LLM:** <span style='color:green'>### YOUR ANSWER ###</span>","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\n# Loading the pre-trained LLM - DistilBERT \nmodel_name = 'distilbert-base-uncased'  \nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  \n\n# Freezing all base model weights\nfor param in model.base_model.parameters():\n    param.requires_grad = False  \nprint(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:20:33.546157Z","iopub.execute_input":"2025-04-09T21:20:33.546475Z","iopub.status.idle":"2025-04-09T21:20:33.776781Z","shell.execute_reply.started":"2025-04-09T21:20:33.546452Z","shell.execute_reply":"2025-04-09T21:20:33.776068Z"}},"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"DistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): DistilBertSdpaAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"   b. Freeze all base model weights and attach a lightweight MLP (the classification head) that maps the modelâ€™s representations to binary labels. You may want to create a separate model class that defines these components and a forward function or use out of the box ðŸ¤— classification wrappers.","metadata":{}},{"cell_type":"code","source":"### ADD YOUR CODE HERE ###\n# Freeze all weights of the loaded LLM\n# Define and attach a lightweight MLP classifier head\n\nimport torch\nfrom torch import nn\nfrom transformers import AutoModel\n\n# Defining the custom model class with the classification head (MLP)\nclass SpamClassifier(nn.Module):\n    def __init__(self, base_model_name='distilbert-base-uncased'):\n        super(SpamClassifier, self).__init__()\n        \n        self.base_model = AutoModel.from_pretrained(base_model_name)\n        \n        for param in self.base_model.parameters():\n            param.requires_grad = False  # Freezing\n\n  \n        self.classifier = nn.Sequential(\n            nn.Linear(self.base_model.config.hidden_size, 64),  \n            nn.ReLU(),\n            nn.Linear(64, 2)  \n        )\n    \n    def forward(self, input_ids, attention_mask):\n  \n        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n        \n        cls_output = outputs.last_hidden_state[:, 0, :]  \n        \n\n        logits = self.classifier(cls_output)\n        \n        return logits\n\n\nspam_classifier = SpamClassifier(base_model_name='distilbert-base-uncased')\n\nprint(spam_classifier)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:20:36.969762Z","iopub.execute_input":"2025-04-09T21:20:36.970043Z","iopub.status.idle":"2025-04-09T21:20:37.193229Z","shell.execute_reply.started":"2025-04-09T21:20:36.970022Z","shell.execute_reply":"2025-04-09T21:20:37.192466Z"}},"outputs":[{"name":"stdout","text":"SpamClassifier(\n  (base_model): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): DistilBertSdpaAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=768, out_features=64, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=64, out_features=2, bias=True)\n  )\n)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"   c. Use the [CLS] token if available or mean-pooled final hidden states from the LLM as input to your classifier head.","metadata":{}},{"cell_type":"code","source":"### ADD YOUR CODE HERE ###\n# Implement logic to extract [CLS] token or mean-pooled hidden states from the LLM's output\n\nimport torch\nfrom torch import nn\nfrom transformers import AutoModel\n\n# Modifying the function to include an option for Mean pooled hidden states and  [CLS] token\n\nclass SpamClassifier(nn.Module):\n    def __init__(self, base_model_name='distilbert-base-uncased', use_mean_pooling=False):\n        super(SpamClassifier, self).__init__()\n        \n    \n        self.base_model = AutoModel.from_pretrained(base_model_name)\n        \n      \n        for param in self.base_model.parameters():\n            param.requires_grad = False  \n\n    \n        self.classifier = nn.Sequential(\n            nn.Linear(self.base_model.config.hidden_size, 64),  \n            nn.ReLU(),\n            nn.Linear(64, 2)  \n        )\n        \n      \n        self.use_mean_pooling = use_mean_pooling\n    \n    def forward(self, input_ids, attention_mask):\n    \n        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n        \n        \n        if not self.use_mean_pooling:\n            cls_output = outputs.last_hidden_state[:, 0, :] \n        else:\n           \n            cls_output = outputs.last_hidden_state.mean(dim=1)\n        \n      \n        logits = self.classifier(cls_output)\n        \n        return logits\n\n\nspam_classifier = SpamClassifier(base_model_name='distilbert-base-uncased', use_mean_pooling=False)\n\n\nprint(spam_classifier)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:20:41.759097Z","iopub.execute_input":"2025-04-09T21:20:41.759384Z","iopub.status.idle":"2025-04-09T21:20:42.050789Z","shell.execute_reply.started":"2025-04-09T21:20:41.759362Z","shell.execute_reply":"2025-04-09T21:20:42.050145Z"}},"outputs":[{"name":"stdout","text":"SpamClassifier(\n  (base_model): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): DistilBertSdpaAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=768, out_features=64, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=64, out_features=2, bias=True)\n  )\n)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"3. Configure your training parameters (learning rate, batch size, epochs) and train the model using only the classifier head while the LLM remains frozen.","metadata":{}},{"cell_type":"code","source":"### ADD YOUR CODE HERE ###\n# Define training parameters (learning rate, batch size, epochs)\n# Define loss function and optimizer (optimize only classifier head parameters)\n# Implement training loop: forward pass, loss calculation, backward pass, optimizer step\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.optim import Adam\nfrom tqdm import tqdm \n\n\nlearning_rate = 2e-5  \nbatch_size = 16       \nepochs = 3            \n\n\ncriterion = torch.nn.CrossEntropyLoss()  \noptimizer = Adam(spam_classifier.classifier.parameters(), lr=learning_rate)  \n\n\ntrain_dataloader = DataLoader(tokenized_train, batch_size=batch_size, shuffle=True)\nval_dataloader = DataLoader(tokenized_val, batch_size=batch_size)\ntest_dataloader = DataLoader(tokenized_test, batch_size=batch_size)\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nspam_classifier.to(device)\n\nfor epoch in range(epochs):\n    spam_classifier.train() \n    total_loss = 0\n    correct_preds = 0\n    total_preds = 0\n    \n    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n       \n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n\n       \n        optimizer.zero_grad()\n\n        \n        outputs = spam_classifier(input_ids, attention_mask)\n        loss = criterion(outputs, labels)\n\n     \n        loss.backward()\n\n      \n        optimizer.step()\n\n      \n        total_loss += loss.item()\n        predictions = torch.argmax(outputs, dim=1)\n        correct_preds += (predictions == labels).sum().item()\n        total_preds += labels.size(0)\n    \n  \n    avg_loss = total_loss / len(train_dataloader)\n    accuracy = correct_preds / total_preds * 100\n\n    print(f\"Epoch {epoch + 1}/{epochs} - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n    \n    \n    spam_classifier.eval() \n    val_loss = 0\n    correct_preds = 0\n    total_preds = 0\n    \n    with torch.no_grad():  \n        for batch in val_dataloader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            \n            outputs = spam_classifier(input_ids, attention_mask)\n            loss = criterion(outputs, labels)\n\n         \n            val_loss += loss.item()\n            predictions = torch.argmax(outputs, dim=1)\n            correct_preds += (predictions == labels).sum().item()\n            total_preds += labels.size(0)\n    \n    avg_val_loss = val_loss / len(val_dataloader)\n    val_accuracy = correct_preds / total_preds * 100\n\n    print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n\n\ntorch.save(spam_classifier.state_dict(), 'spam_classifier_model.pth')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:20:46.676195Z","iopub.execute_input":"2025-04-09T21:20:46.676519Z","iopub.status.idle":"2025-04-09T21:47:58.752450Z","shell.execute_reply.started":"2025-04-09T21:20:46.676480Z","shell.execute_reply":"2025-04-09T21:47:58.751761Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1784/1784 [08:05<00:00,  3.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - Loss: 0.4293, Accuracy: 89.88%\nValidation Loss: 0.2492, Validation Accuracy: 92.88%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1784/1784 [08:11<00:00,  3.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/3 - Loss: 0.1963, Accuracy: 94.45%\nValidation Loss: 0.1551, Validation Accuracy: 95.24%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1784/1784 [08:13<00:00,  3.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/3 - Loss: 0.1450, Accuracy: 95.35%\nValidation Loss: 0.1295, Validation Accuracy: 95.59%\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"4. Evaluation and Analysis:","metadata":{}},{"cell_type":"markdown","source":"   a. Evaluate the model on the test set using accuracy, precision, recall, and F1-score.","metadata":{}},{"cell_type":"code","source":"### ADD YOUR CODE HERE ###\n# Evaluate the trained model on the test set\n# Calculate and report accuracy, precision, recall, and F1-score\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom torch.utils.data import DataLoader\nimport torch\nfrom tqdm import tqdm\n\n\ntest_dataloader = DataLoader(tokenized_test, batch_size=16)\n\n\nspam_classifier.eval()\n\n\nall_preds = []\nall_labels = []\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nspam_classifier.to(device)\n\n\nwith torch.no_grad():  \n    for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n       \n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n        \n        \n        outputs = spam_classifier(input_ids=input_ids, attention_mask=attention_mask)\n        \n       \n        logits = outputs  \n        \n        \n        preds = torch.argmax(logits, dim=1)\n\n      \n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n\naccuracy = accuracy_score(all_labels, all_preds)\nprecision = precision_score(all_labels, all_preds)\nrecall = recall_score(all_labels, all_preds)\nf1 = f1_score(all_labels, all_preds)\n\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:48:44.933365Z","iopub.execute_input":"2025-04-09T21:48:44.933723Z","iopub.status.idle":"2025-04-09T21:49:20.008819Z","shell.execute_reply.started":"2025-04-09T21:48:44.933658Z","shell.execute_reply":"2025-04-09T21:49:20.007659Z"}},"outputs":[{"name":"stderr","text":"Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:35<00:00,  3.57it/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.9580\nPrecision: 0.9676\nRecall: 0.9484\nF1 Score: 0.9579\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"   b. Select **two** encoder LLMs, repeat steps 2-4 for the second LLM, and compare and discuss any performance trends between the two models. **Specify the second chosen LLM below and report performance comparison.**","metadata":{}},{"cell_type":"markdown","source":"   **Second Chosen Encoder LLM:**  TinyBERT Model\n\n   Reason for choosing the TinyBERT Model \n\n   **TinyBERT** is a great choice because it's a compact, efficient model optimized for quicker inference and lower resource usage, yet still performs well on many tasks. Comparing it with a larger model like DistilBERT helps highlight the balance between speed and accuracy.\n","metadata":{}},{"cell_type":"markdown","source":"Defining the Second LLM Model and repeating the above steps mentioned and performed on distilbert","metadata":{}},{"cell_type":"code","source":"from transformers import AdamW\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:53:04.719778Z","iopub.execute_input":"2025-04-09T21:53:04.720083Z","iopub.status.idle":"2025-04-09T21:53:04.756254Z","shell.execute_reply.started":"2025-04-09T21:53:04.720060Z","shell.execute_reply":"2025-04-09T21:53:04.755618Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"\nmodel_name_tinybert = 'huawei-noah/TinyBERT_General_4L_312D'  \nmodel_tinybert = AutoModelForSequenceClassification.from_pretrained(model_name_tinybert, num_labels=2) \n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:52:53.559197Z","iopub.execute_input":"2025-04-09T21:52:53.559512Z","iopub.status.idle":"2025-04-09T21:52:54.108866Z","shell.execute_reply.started":"2025-04-09T21:52:53.559487Z","shell.execute_reply":"2025-04-09T21:52:54.107989Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"for param in model_tinybert.base_model.parameters():\n    param.requires_grad = False  \n\n\nclass SpamClassifierTinyBERT(nn.Module):\n    def __init__(self, base_model_name='huawei-noah/TinyBERT_General_4L_312D', use_mean_pooling=False):\n        super(SpamClassifierTinyBERT, self).__init__()\n        \n        \n        self.base_model = AutoModel.from_pretrained(base_model_name)\n        \n        \n        for param in self.base_model.parameters():\n            param.requires_grad = False \n\n        self.classifier = nn.Sequential(\n            nn.Linear(self.base_model.config.hidden_size, 64), \n            nn.ReLU(),\n            nn.Linear(64, 2)  \n        )\n        \n        self.use_mean_pooling = use_mean_pooling\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n    \n        if not self.use_mean_pooling:\n            cls_output = outputs.last_hidden_state[:, 0, :] \n        else:\n            cls_output = outputs.last_hidden_state.mean(dim=1)\n        \n\n        logits = self.classifier(cls_output)\n        \n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:53:25.418893Z","iopub.execute_input":"2025-04-09T21:53:25.419177Z","iopub.status.idle":"2025-04-09T21:53:25.425947Z","shell.execute_reply.started":"2025-04-09T21:53:25.419156Z","shell.execute_reply":"2025-04-09T21:53:25.425042Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"TinyBERT's Model Architecture ","metadata":{}},{"cell_type":"code","source":"\nspam_classifier_tinybert = SpamClassifierTinyBERT(base_model_name='huawei-noah/TinyBERT_General_4L_312D', use_mean_pooling=False)\nprint(spam_classifier_tinybert)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:53:35.474970Z","iopub.execute_input":"2025-04-09T21:53:35.475244Z","iopub.status.idle":"2025-04-09T21:53:35.983974Z","shell.execute_reply.started":"2025-04-09T21:53:35.475223Z","shell.execute_reply":"2025-04-09T21:53:35.982876Z"}},"outputs":[{"name":"stdout","text":"SpamClassifierTinyBERT(\n  (base_model): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 312, padding_idx=0)\n      (position_embeddings): Embedding(512, 312)\n      (token_type_embeddings): Embedding(2, 312)\n      (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-3): 4 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=312, out_features=312, bias=True)\n              (key): Linear(in_features=312, out_features=312, bias=True)\n              (value): Linear(in_features=312, out_features=312, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=312, out_features=312, bias=True)\n              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=312, out_features=1200, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=1200, out_features=312, bias=True)\n            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=312, out_features=312, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=312, out_features=64, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=64, out_features=2, bias=True)\n  )\n)\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"### ADD YOUR CODE HERE ###\n# Repeat steps 2-4 for the second chosen LLM\n# Implement code for performance comparison and trend analysis\n\n\n\nlearning_rate = 1e-5\nbatch_size = 16\nepochs = 3\noptimizer_tinybert = AdamW(spam_classifier_tinybert.parameters(), lr=learning_rate)\nloss_fn_tinybert = nn.CrossEntropyLoss()\n\n\nspam_classifier_tinybert.to(device)\n\n# Training the TinyBERT model \nfor epoch in range(epochs):\n    spam_classifier_tinybert.train()\n    total_loss = 0\n    total_preds = 0\n\n    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n\n      \n        optimizer_tinybert.zero_grad()\n        outputs = spam_classifier_tinybert(input_ids=input_ids, attention_mask=attention_mask)\n        loss = loss_fn_tinybert(outputs, labels)\n\n       \n        loss.backward()\n        optimizer_tinybert.step()\n\n        total_loss += loss.item()\n        preds = torch.argmax(outputs, dim=1)\n        total_preds += torch.sum(preds == labels).item()\n\n    avg_loss = total_loss / len(train_dataloader)\n    accuracy = total_preds / len(train_dataloader.dataset)\n\n    print(f\"Epoch {epoch + 1}/{epochs} - Loss: {avg_loss:.4f} - Accuracy: {accuracy:.4f}\")\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:53:45.465840Z","iopub.execute_input":"2025-04-09T21:53:45.466134Z","iopub.status.idle":"2025-04-09T21:58:14.510105Z","shell.execute_reply.started":"2025-04-09T21:53:45.466111Z","shell.execute_reply":"2025-04-09T21:58:14.509161Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nEpoch 1/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1784/1784 [01:29<00:00, 19.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - Loss: 0.6746 - Accuracy: 0.6579\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1784/1784 [01:29<00:00, 19.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/3 - Loss: 0.6312 - Accuracy: 0.7435\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1784/1784 [01:30<00:00, 19.82it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 3/3 - Loss: 0.5844 - Accuracy: 0.7621\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# Evaluate TinyBERT model on the test set\ntest_dataloader = DataLoader(tokenized_test, batch_size=batch_size)\n\nspam_classifier_tinybert.eval()\nall_preds_tinybert = []\nall_labels_tinybert = []\n\nwith torch.no_grad():\n    for batch in tqdm(test_dataloader, desc=\"Evaluating TinyBERT\"):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n\n        outputs = spam_classifier_tinybert(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs\n        preds = torch.argmax(logits, dim=1)\n\n        all_preds_tinybert.extend(preds.cpu().numpy())\n        all_labels_tinybert.extend(labels.cpu().numpy())\n\naccuracy_tinybert = accuracy_score(all_labels_tinybert, all_preds_tinybert)\nprecision_tinybert = precision_score(all_labels_tinybert, all_preds_tinybert)\nrecall_tinybert = recall_score(all_labels_tinybert, all_preds_tinybert)\nf1_tinybert = f1_score(all_labels_tinybert, all_preds_tinybert)\n\nprint(f\"TinyBERT Evaluation Results:\")\nprint(f\"Accuracy: {accuracy_tinybert:.4f}\")\nprint(f\"Precision: {precision_tinybert:.4f}\")\nprint(f\"Recall: {recall_tinybert:.4f}\")\nprint(f\"F1 Score: {f1_tinybert:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T22:00:10.178218Z","iopub.execute_input":"2025-04-09T22:00:10.178630Z","iopub.status.idle":"2025-04-09T22:00:15.976412Z","shell.execute_reply.started":"2025-04-09T22:00:10.178599Z","shell.execute_reply":"2025-04-09T22:00:15.975595Z"}},"outputs":[{"name":"stderr","text":"Evaluating TinyBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:05<00:00, 21.65it/s]","output_type":"stream"},{"name":"stdout","text":"TinyBERT Evaluation Results:\nAccuracy: 0.7790\nPrecision: 0.8083\nRecall: 0.7361\nF1 Score: 0.7705\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"   **Performance Comparison and Trend Discussion:**","metadata":{}},{"cell_type":"markdown","source":"<span style='color:green'>### YOUR ANSWER ###</span>","metadata":{}},{"cell_type":"markdown","source":"\nBelow is a comparison of the performance metrics of the two models evaluated: \n\n\n**TinyBERT Evaluation Results:**\n\nAccuracy: 0.7790\n\n\nPrecision: 0.8083\n\n\nRecall: 0.7361\n\n\nF1 Score: 0.7705\n\n\n**DistilBERT Evaluation Results:**\n\nAccuracy: 0.9580\n\n\nPrecision: 0.9676\n\n\nRecall: 0.9484\n\n\nF1 Score: 0.9579","metadata":{}},{"cell_type":"markdown","source":"**Trend Analysis:**\nTinyBERT is more efficient and lighter, and although it gets good performance for a smaller model, it is not outperformed by DistilBERT on this task. The smaller model architecture of TinyBERT is achieved at the expense of efficiency against model performance and is evident through lower accuracy, precision, recall, and F1 score.\n\n\nDistilBERT is a more powerful model, and on this task, its performance is that it can capture more complicated features of the text, leading to significantly better results. The model achieves accuracy of 95.8%, well ahead of TinyBERT results and comfortably surpassing target levels.","metadata":{}},{"cell_type":"markdown","source":"   c. The best model is expected to attain {Accuracy: >85%, F1: >85%, Precision: >85%, Recall: >82%}. Report whether your best model achieves these metrics and discuss.","metadata":{}},{"cell_type":"markdown","source":"\nThe DistilBERT model is the best model which exceeds all the expected metrics:\n\nAccuracy: >85% (achieved 95.8%)\n\nPrecision: >85% (achieved 96.76%)\n\nRecall: >82% (achieved 94.84%)\n\nF1 Score: >85% (achieved 95.79%)\n\nTherefore, the best model (DistilBERT) surpasses and fulfills the target performance levels, i.e., it is highly efficient for this task.\n\n\n- DistilBERT is highly efficient due to its larger structure compared to TinyBERT, and hence it can learn more complex patterns and representations from the text.\n\n- DistilBERT is a BERT distill but retains nearly all of BERT's language capabilities but with reduced resource use. It has the ability to examine context information to a higher degree of profundity in order to make more accurate predictions.\n\n- Furthermore, the pre-training of the model on a large corpus of data enables it to take advantage of learned patterns in language, resulting in better generalization on spam classification tasks, for example. This synergy of depth, efficiency, and pre-training is what makes it highly performant across all the important metrics.","metadata":{}},{"cell_type":"markdown","source":"   **Performance vs. Expected Metrics Discussion:**","metadata":{}},{"cell_type":"markdown","source":"<span style='color:green'>### YOUR ANSWER ###</span>","metadata":{}},{"cell_type":"markdown","source":"**The expected performance metrics are:**\n\nAccuracy: >85%\nPrecision: >85%\nRecall: >82%\nF1 Score: >85%\n\n**TinyBERT Model:**\n\nAccuracy: 77.9\n\nTinyBERT is below the target accuracy which is >85%. It is poor in its performance since it is a small model in size along with fewer parameters than the larger models such as DistilBERT. In terms of efficiency, it fell short of encompassing as much complexity and therefore is less accurate.\n\nAccuracy: 80.83\n\nAccuracy of TinyBERT is average but not yet reaching the best >85%. I.e., the model is very good at identifying the positive class but possibly still producing some false positives.\n\nRecall: 73.61\n\nThe recall is subpar. With >82% being the goal, TinyBERT misses out on significant amounts of true positives, which in a spam detection task where we need to catch as many spam messages as possible, could be significant.\n\nF1 Score: 77.05%\n\nThe F1 score is still under par. While precision vs. recall trade-off is good, TinyBERT's overall performance is still lower compared to more computationally heavy models.\n\n**DistilBERT Model:**\n\nAccuracy: 95.8%\n\nThe DistilBERT model comfortably breaches the >85% accuracy mark, showcasing its capability to classify text with high accuracy reliably.\n\nPrecision: 96.76%\n\nDistilBERT's precision is superb, i.e., it is very good at identifying the positive class with virtually no false positives, far better than the >85% level.\n\nRecall: 94.84%\n\nRecall is also excellent, well over baseline >82%. This indicates that the model is highly effective at capturing the true positive instances, which is highly relevant for uses like spam filtering.\n\nF1 Score: 95.79%\n\nDistilBERT's F1 score is also excellent, further supporting its overall performance. With an F1 score considerably higher than 85%, the model has a very good precision-recall balance.\n\n**Conclusion:**\n\nBased on the evaluation metrics, DistilBERT performs the best. It exceeds all the target values of accuracy, precision, recall, and F1 score. It is clearly more appropriate for this task since it's larger and able to capture more subtle patterns in the text.\n\n\nTinyBERT does worse on this task, but as it is a lightweight model, it is an apt choice when high performance is not as desirable as efficiency. It has a smaller size, which results in faster inference, hence being better suited for environments with constrained resources. In performance, however, it cannot catch up to the larger DistilBERT model.","metadata":{}},{"cell_type":"markdown","source":"5. References. Include details on all the resources used to complete this part.","metadata":{}},{"cell_type":"markdown","source":"<span style='color:green'>### YOUR ANSWER ###</span>","metadata":{}},{"cell_type":"markdown","source":"https://medium.com/@azimkhan8018/email-spam-detection-with-machine-learning-a-comprehensive-guide-b65c6936678b\n\nhttps://www.kaggle.com/datasets/balaka18/email-spam-classification-dataset-csv\n\nhttps://www.kaggle.com/datasets/tapakah68/email-spam-classification/code\n\nhttps://link.springer.com/article/10.1007/s10207-023-00756-1\n\nhttps://hugobowne.github.io/hugo-blog/posts/fine-tuning-llms-gpt-2/\n\nhttps://huggingface.co/datasets/SetFit/enron_spam\n\nhttps://heartbeat.comet.ml/using-transfer-learning-and-pre-trained-language-models-to-classify-spam-549fc0f56c20\n\nhttps://blog.madhukaraphatak.com/bert-email-spam-1\n\nhttps://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/\n\nhttps://medium.com/@varun.tyagi83/introducing-the-spam-detection-model-with-pre-trained-llm-3eb1f8186ba1\n\n","metadata":{}}]}